{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e71e4-f83a-4e26-98c0-d786de085570",
   "metadata": {},
   "source": [
    "### Read Story as input text into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae08f76-6257-4958-a0b9-da34e63c8a80",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed01f8d0-1cfc-43ce-a8fc-6c1a57e0e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total No of Characters: \" ,len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4179-fe12-4512-8ac4-1d07c62c11ad",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">Our goal is to tokenize this 20479 into short story and the goal is to convert and </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b585a-0e56-44b9-8af7-e9a8e795a330",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">We will be using regular expression python library to split the text </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23f3745-714f-4961-b75b-943fdaf12554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This is a text\";\n",
    "result = re.split(r'(\\s)',text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d70e-216e-4221-b69a-e3fd6c4bb8a8",
   "metadata": {},
   "source": [
    "The result of this is whitespaces and punctuation characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387e50c-57a0-4e23-bfa8-d5edac4b4bde",
   "metadata": {},
   "source": [
    "Now we want to remove comma, spaces and fullstops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f5867e-ae87-43fc-9a7d-fb4f283c5bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([.,]|\\s)',text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591671-438d-4700-bf19-a9d01c72b667",
   "metadata": {},
   "source": [
    "Whitespace characters are still present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be665fd-37fb-491a-abd3-d8dd81d4b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'text']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f535-686e-464e-849c-a46e29cb6650",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159e2ce4-4b57-48c8-b03d-a6df3bee74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!', '.', 'Is', 'this', '--', 'a', 'text', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World!. Is this-- a  text?\"\n",
    "result = re.split(r'([,.!?:;\"\\']|--|\\s)',text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3241b6-fb10-44db-b7c2-d44c5cb62556",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> Now as we got the basic tokenzier working, lets apply to story</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0792ad-3423-4d99-9122-96550622cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ca13d1d-f432-42a9-83e7-25d77811c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b7a4e-bb9f-43af-a7ad-f86c9a57697f",
   "metadata": {},
   "source": [
    "# Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786e9aa-ac72-449f-9469-5f9c6e186a8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> Vocabulary is like a dictionay, associated with tokens and token Id. Below we are storing it in a set and sorting accordingly</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57268a7b-7b6b-4ff0-b9c4-d75a3b8ec775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f8d60c-781b-4e58-89d9-0ebf20d208b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "babb24d4-8009-4220-9300-4426e875e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867cc2c-180a-4488-9537-6f3a65d63beb",
   "metadata": {},
   "source": [
    "As we can see, it has unique values and intgers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc3114-33a8-4702-b917-37fd54eb286d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4899cc9-45ab-4df0-be75-86a00ae597f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422dfa4-dc22-4ce7-b06b-cded920580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953fc6c-d592-45a4-8097-8c027fe3c14e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aabd38c-008f-42cb-9b2b-3005c05579cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4a9a7-be48-4414-aa64-09c4a3568eed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6a330bc-aa0e-4c80-99bb-e367d0185018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c2bfe0c-f31b-4981-979f-f29754ea841b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text1 = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "tokenizer.encode(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41b022-e917-41c1-9af5-a7178abeb2b6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4e2ef-557b-4fbf-b6a9-874fb550b553",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd019cee-94ab-4b56-a17a-0b634d0c2ec1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76ce6506-2f0a-4216-bd0e-f5308c5b5b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "872d8374-2bf6-44dd-bc03-bd4c30e94d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a1b9e-9ba4-4025-b70c-c76522c182c6",
   "metadata": {},
   "source": [
    "Now it will handle the unknown words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88bb7091-d51f-45d1-a9e8-c0efd7ff48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eb8d248-29f9-4831-915c-6cd8a9b01d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97a0d959-0b05-46d9-a158-5364fcbeaa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a87dcd90-dd9e-49b7-b42a-fbf1a85fc021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afeed94-fc59-4b74-b296-8001bef08534",
   "metadata": {},
   "source": [
    "GPT uses mostly endoftext token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a2094-168c-481c-b4e7-c105410fd24b",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91d3cf6-6181-494a-97d6-09721fd19946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vipulgirme/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca3b2d-eaaf-4d6a-ac04-2c80ab395c13",
   "metadata": {},
   "source": [
    "tittoken is a python lib which helps us build Byte Pair token as the algorithm is complicated, the same is being used by ChatGPT 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9d9c42e-dff5-48b9-bd27-0d5bfdf55ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python\n",
      "['/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python313.zip', '/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13', '/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload', '', '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages', '/opt/homebrew/opt/certifi/lib/python3.13/site-packages', '/opt/homebrew/lib/python3.13/site-packages', '/Users/vipulgirme/Library/Python/3.9/lib/python/site-packages', '/Users/vipulgirme/Library/Python/3.9/lib/python/site-packages', '/Users/vipulgirme/Library/Python/3.9/lib/python/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a41f45-ac62-48d7-b62d-535c9a199fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "  Downloading regex-2024.11.6.tar.gz (399 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.9.0.tar.gz (35 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.32.3.tar.gz (131 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading charset_normalizer-3.4.2.tar.gz (126 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading idna-3.10.tar.gz (190 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading urllib3-2.4.0.tar.gz (390 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading certifi-2025.4.26.tar.gz (160 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: regex, tiktoken, requests, certifi, charset_normalizer, idna, urllib3\n",
      "  Building wheel for regex (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2024.11.6-cp313-cp313-macosx_15_0_arm64.whl size=285568 sha256=ec38379ed137224d14aff7bcf47875637f98503e08d1e29dbbd34fcc3c2ab17d\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/78/25/fd/78977b709cb576063310b902b1a828bbb1c5f62bd4d2890332\n",
      "  Building wheel for tiktoken (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tiktoken: filename=tiktoken-0.9.0-cp313-cp313-macosx_15_0_arm64.whl size=985395 sha256=6ddafd86d31cc7ae637991f22e69b18d678a5b0ef78a12a6ffba406f9d35ffc1\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/11/ef/d9/b1e88c2b54a7410a30fc41728ff8c4cb07241fa535ade0820d\n",
      "  Building wheel for requests (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for requests: filename=requests-2.32.3-py3-none-any.whl size=65026 sha256=de960d56c24f6228fb810cb1d5ce6cd55fc4b11683f8ea060f17e8c77d6bb4a6\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/92/64/3e/1732559baed84e69b54adc9a34410a7118416551e3c609b4ef\n",
      "  Building wheel for certifi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for certifi: filename=certifi-2025.4.26-py3-none-any.whl size=159619 sha256=bcd1910a4da26c3c004cd341400383ee8dcbd1bc3e827e3ebab54143b7f37131\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/34/9c/58/cda09f3752000c484808e507a47f1eda813ee4d0d75ed4884f\n",
      "  Building wheel for charset_normalizer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for charset_normalizer: filename=charset_normalizer-3.4.2-py3-none-any.whl size=52628 sha256=78b8c0382daa41ff8509af0013a2a8ba0ac0a06510558a48839ea728a3cbad61\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/78/b3/cf/91d31f17365250421e6b3d53632c13beb26117481d7f9ead27\n",
      "  Building wheel for idna (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna: filename=idna-3.10-py3-none-any.whl size=70489 sha256=36a7e3b0616c432f5211e9ec650ca0867991970beb214b3dd73c3bc948a8b920\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/13/72/f8/cd5b0bba8c24bed053d8c1659667d8d013ba7b311d41b31f85\n",
      "  Building wheel for urllib3 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for urllib3: filename=urllib3-2.4.0-py3-none-any.whl size=128680 sha256=f9d992b01ce349f718f5ba7d157a775325808c786351327b777825b64bc6a742\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-b7mfbltw/wheels/9a/1c/50/aa867d1b209235d1985bc828eff7f793755000b3caa23705a0\n",
      "Successfully built regex tiktoken requests certifi charset_normalizer idna urllib3\n",
      "Installing collected packages: urllib3, regex, idna, charset_normalizer, certifi, requests, tiktoken\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall urllib3 2.3.0\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for urllib3.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: The package was installed by brew. You should check if it can uninstall the package.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --force-reinstall --no-binary :all: --no-cache-dir regex tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d17b25f9-fab8-4550-9dfb-85336479e61a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtiktoken version:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.9/lib/python/site-packages/tiktoken/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This is the public API of tiktoken\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Encoding \u001b[38;5;28;01mas\u001b[39;00m Encoding\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encoding_for_model \u001b[38;5;28;01mas\u001b[39;00m encoding_for_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encoding_name_for_model \u001b[38;5;28;01mas\u001b[39;00m encoding_name_for_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.9/lib/python/site-packages/tiktoken/core.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, AbstractSet, Collection, Literal, NoReturn, Sequence\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tiktoken\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'regex'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27703fcb-6b74-47f8-8929-cb8b9a96db17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tiktoken as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping regex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tiktoken\n",
      "  Downloading tiktoken-0.9.0.tar.gz (35 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2024.11.6.tar.gz (399 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Building wheels for collected packages: tiktoken, regex\n",
      "  Building wheel for tiktoken (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tiktoken: filename=tiktoken-0.9.0-cp313-cp313-macosx_15_0_arm64.whl size=985395 sha256=96ea944bc79cdcc73d1b40667053d9bb042745010c2f9caa9cb239cd83869134\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-1zh199mb/wheels/11/ef/d9/b1e88c2b54a7410a30fc41728ff8c4cb07241fa535ade0820d\n",
      "  Building wheel for regex (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2024.11.6-cp313-cp313-macosx_15_0_arm64.whl size=285570 sha256=32a7c8bfb6826c5ffd8f17e293bc042f827b4b5b47dc7a7168175d18fc1303a2\n",
      "  Stored in directory: /private/var/folders/ld/c99syxqd1kxdr2j70t3cwctc0000gn/T/pip-ephem-wheel-cache-1zh199mb/wheels/78/25/fd/78977b709cb576063310b902b1a828bbb1c5f62bd4d2890332\n",
      "Successfully built tiktoken regex\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y tiktoken regex\n",
    "!{sys.executable} -m pip install --no-cache-dir --no-binary :all: tiktoken regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a82b950a-c708-4b79-b2cc-a43ccce2b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ffc0281-fe05-4bc4-8c0c-4ad98395baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d172978-6c65-4333-8b33-a79f46742da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba3dd08-b070-48ff-a882-da9b916c3b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5aefa7-0a55-4db3-8d57-b42fa784d0f0",
   "metadata": {},
   "source": [
    "Let's try some random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5553269c-f317-4100-933f-573563a2ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 478, 495, 807, 25, 383, 402, 11571, 29130, 7509, 25, 30589, 39645, 14711, 7656]\n",
      "Lecture 8: The GPT Tokenizer: Byte Pair Encoding\n"
     ]
    }
   ],
   "source": [
    "text = (\"Lecture 8: The GPT Tokenizer: Byte Pair Encoding\") \n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e07f02-e472-4aa6-a053-3c68dcd21f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
