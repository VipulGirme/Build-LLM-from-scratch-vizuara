{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e71e4-f83a-4e26-98c0-d786de085570",
   "metadata": {},
   "source": [
    "### Read Story as input text into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae08f76-6257-4958-a0b9-da34e63c8a80",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed01f8d0-1cfc-43ce-a8fc-6c1a57e0e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of Characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total No of Characters: \" ,len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4179-fe12-4512-8ac4-1d07c62c11ad",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">Our goal is to tokenize this 20479 into short story and the goal is to convert and </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b585a-0e56-44b9-8af7-e9a8e795a330",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">We will be using regular expression python library to split the text </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23f3745-714f-4961-b75b-943fdaf12554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This is a text\";\n",
    "result = re.split(r'(\\s)',text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d70e-216e-4221-b69a-e3fd6c4bb8a8",
   "metadata": {},
   "source": [
    "The result of this is whitespaces and punctuation characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387e50c-57a0-4e23-bfa8-d5edac4b4bde",
   "metadata": {},
   "source": [
    "Now we want to remove comma, spaces and fullstops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f5867e-ae87-43fc-9a7d-fb4f283c5bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([.,]|\\s)',text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591671-438d-4700-bf19-a9d01c72b667",
   "metadata": {},
   "source": [
    "Whitespace characters are still present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be665fd-37fb-491a-abd3-d8dd81d4b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'text']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f535-686e-464e-849c-a46e29cb6650",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159e2ce4-4b57-48c8-b03d-a6df3bee74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!', '.', 'Is', 'this', '--', 'a', 'text', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World!. Is this-- a  text?\"\n",
    "result = re.split(r'([,.!?:;\"\\']|--|\\s)',text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3241b6-fb10-44db-b7c2-d44c5cb62556",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> Now as we got the basic tokenzier working, lets apply to story</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0792ad-3423-4d99-9122-96550622cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ca13d1d-f432-42a9-83e7-25d77811c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b7a4e-bb9f-43af-a7ad-f86c9a57697f",
   "metadata": {},
   "source": [
    "# Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786e9aa-ac72-449f-9469-5f9c6e186a8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> Vocabulary is like a dictionay, associated with tokens and token Id. Below we are storing it in a set and sorting accordingly</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57268a7b-7b6b-4ff0-b9c4-d75a3b8ec775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f8d60c-781b-4e58-89d9-0ebf20d208b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "babb24d4-8009-4220-9300-4426e875e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867cc2c-180a-4488-9537-6f3a65d63beb",
   "metadata": {},
   "source": [
    "As we can see, it has unique values and intgers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc3114-33a8-4702-b917-37fd54eb286d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4899cc9-45ab-4df0-be75-86a00ae597f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1bbb50-c26e-4509-b210-798a0ce35b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
